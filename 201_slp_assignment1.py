# -*- coding: utf-8 -*-
"""201 - SLP Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UhPKqM5Jc7mbaGCSiV0wV-LV5SIvN6uR
"""

pip install --upgrade gensim

# Import necessary libraries
import nltk
import gensim
import matplotlib.pyplot as plt
import seaborn as sns
from gensim.models import Word2Vec
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# Define training data (list of tokenized sentences)
sentences = [
    ['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],
    ['this', 'is', 'the', 'second', 'sentence'],
    ['yet', 'another', 'sentence'],
    ['one', 'more', 'sentence'],
    ['and', 'the', 'final', 'sentence']
]

# Train Word2Vec model
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# Print the trained model summary
print("\n‚úÖ Trained Model:", model)

# Summarize vocabulary
words = list(model.wv.index_to_key)  # Get all words in the vocabulary
print("\nüìå Vocabulary:", words)

# Access the vector for a specific word
word_vector = model.wv['sentence']
print("\nüî¢ Vector for 'sentence':", word_vector[:10])  # Print first 10 values for readability

# Save the model
model.save("word2vec_model.bin")
print("\nüíæ Model saved successfully!")

# Load the saved model
loaded_model = Word2Vec.load("word2vec_model.bin")
print("\n‚úÖ Loaded Model:", loaded_model)

# ---- 1Ô∏è‚É£ Checking the Quality of Word Embeddings ----
print("\nüìä Word Similarities:")

# Find words similar to "sentence"
similar_words = model.wv.most_similar('sentence', topn=5)
print("\nüîç Words similar to 'sentence':", similar_words)

# Check similarity between two words
similarity = model.wv.similarity('this', 'is')
print("\nü§ù Similarity between 'this' and 'is':", similarity)

# ---- 2Ô∏è‚É£ Visualization using PCA ----

# Extract word vectors for all words
word_vectors = model.wv[words]

# Apply PCA to reduce dimensionality from 100 to 2
pca = PCA(n_components=2)
word_vectors_2d = pca.fit_transform(word_vectors)

# Plot the word embeddings
plt.figure(figsize=(8, 6))
plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1])

# Annotate each point with the corresponding word
for i, word in enumerate(words):
    plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]))

plt.title("üìå Word Embeddings Visualization using PCA")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.grid()
plt.show()

# ---- 3Ô∏è‚É£ Visualization using t-SNE ----

# Apply t-SNE to reduce dimensionality from 100 to 2
tsne = TSNE(n_components=2, perplexity=3, random_state=42)
word_vectors_tsne = tsne.fit_transform(word_vectors)

# Plot the t-SNE visualization
plt.figure(figsize=(8, 6))
sns.scatterplot(x=word_vectors_tsne[:, 0], y=word_vectors_tsne[:, 1])

# Annotate each point with the corresponding word
for i, word in enumerate(words):
    plt.annotate(word, xy=(word_vectors_tsne[i, 0], word_vectors_tsne[i, 1]))

plt.title("üìå Word Embeddings Visualization using t-SNE")
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")
plt.grid()
plt.show()

# ---- 4Ô∏è‚É£ Evaluate the Quality of Embeddings ----

# Intrinsic Evaluation: Word Similarity
print("\nüìä Intrinsic Evaluation: Word Similarity")

# Find words similar to "sentence"
similar_words = model.wv.most_similar('sentence', topn=5)
print("\nüîç Words similar to 'sentence':", similar_words)

# Check similarity between two words
similarity = model.wv.similarity('this', 'is')
print("\nü§ù Similarity between 'this' and 'is':", similarity)

# Intrinsic Evaluation: Word Analogies
print("\nüìä Intrinsic Evaluation: Word Analogies")

# Example analogy: king - man + woman = queen
try:
    analogy_result = model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)
    print("\nüîç Analogy result for 'king - man + woman':", analogy_result)
except KeyError as e:
    print(f"\n‚ö†Ô∏è Error: {e}. Some words in the analogy are not in the vocabulary.")

# Extrinsic Evaluation: Downstream Task (Example: Text Classification)
# Note: This is a placeholder for a downstream task. You would need a labeled dataset for actual evaluation.
print("\nüìä Extrinsic Evaluation: Downstream Task (Placeholder)")
print("For extrinsic evaluation, use the embeddings in tasks like text classification or sentiment analysis.")

# Visual Inspection: Clustering
print("\nüìä Visual Inspection: Clustering")

# Plot the t-SNE visualization again for reference
plt.figure(figsize=(8, 6))
sns.scatterplot(x=word_vectors_tsne[:, 0], y=word_vectors_tsne[:, 1])

# Annotate each point with the corresponding word
for i, word in enumerate(words):
    plt.annotate(word, xy=(word_vectors_tsne[i, 0], word_vectors_tsne[i, 1]))

plt.title("üìå Word Embeddings Visualization using t-SNE")
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")
plt.grid()
plt.show()